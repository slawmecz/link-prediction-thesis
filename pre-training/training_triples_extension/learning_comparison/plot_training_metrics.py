#!/usr/bin/env python
"""
Script to plot and compare training metrics between baseline and bidirectional ComplEx models.
Reads CSV files generated by the callback-based training scripts.
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import argparse

# Set style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_training_metrics(baseline_csv: str, bidirectional_csv: str):
    """Load training metrics from CSV files."""
    
    baseline_df = None
    bidirectional_df = None
    
    if os.path.exists(baseline_csv):
        baseline_df = pd.read_csv(baseline_csv)
        print(f"Loaded baseline metrics: {len(baseline_df)} epochs")
    else:
        print(f"Warning: Baseline CSV not found: {baseline_csv}")
    
    if os.path.exists(bidirectional_csv):
        bidirectional_df = pd.read_csv(bidirectional_csv)
        print(f"Loaded bidirectional metrics: {len(bidirectional_df)} epochs")
    else:
        print(f"Warning: Bidirectional CSV not found: {bidirectional_csv}")
    
    return baseline_df, bidirectional_df

def plot_metric_comparison(baseline_df, bidirectional_df, metric_name, ax, ylabel=None):
    """Plot comparison of a single metric."""
    
    if ylabel is None:
        ylabel = metric_name
    
    # Plot baseline
    if baseline_df is not None and metric_name in baseline_df.columns:
        ax.plot(baseline_df['epoch'], baseline_df[metric_name], 
                'o-', color='blue', linewidth=2, markersize=4, 
                label='Baseline', alpha=0.8)
    
    # Plot bidirectional
    if bidirectional_df is not None and metric_name in bidirectional_df.columns:
        ax.plot(bidirectional_df['epoch'], bidirectional_df[metric_name], 
                's-', color='orange', linewidth=2, markersize=4, 
                label='Bidirectional', alpha=0.8)
    
    ax.set_xlabel('Epoch')
    ax.set_ylabel(ylabel)
    ax.set_title(f'{ylabel} vs. Epoch')
    ax.legend()
    ax.grid(True, alpha=0.3)

def create_training_comparison_plots(baseline_csv: str, bidirectional_csv: str, output_dir: str = "metrics_plots"):
    """Create comprehensive training comparison plots."""
    
    print("=== Training Metrics Comparison Plots ===")
    
    # Load data
    baseline_df, bidirectional_df = load_training_metrics(baseline_csv, bidirectional_csv)
    
    if baseline_df is None and bidirectional_df is None:
        print("Error: No valid CSV files found!")
        return
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Define metrics to plot
    metrics_to_plot = [
        ('Hits@1', 'Hits@1'),
        ('Hits@3', 'Hits@3'), 
        ('Hits@5', 'Hits@5'),
        ('Hits@10', 'Hits@10'),
        ('MRR', 'Mean Reciprocal Rank'),
        ('Mean_Rank', 'Mean Rank')
    ]
    
    # Create subplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ComplEx Training Metrics: Baseline vs. Bidirectional', fontsize=16, fontweight='bold')
    
    # Plot each metric
    for i, (metric_col, metric_title) in enumerate(metrics_to_plot):
        row = i // 3
        col = i % 3
        ax = axes[row, col]
        
        plot_metric_comparison(baseline_df, bidirectional_df, metric_col, ax, ylabel=metric_title)
    
    plt.tight_layout()
    
    # Save plot
    plot_file = os.path.join(output_dir, 'training_metrics_comparison.png')
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    print(f"Saved comparison plot to: {plot_file}")
    
    # Also save as PDF
    pdf_file = os.path.join(output_dir, 'training_metrics_comparison.pdf')
    plt.savefig(pdf_file, bbox_inches='tight')
    print(f"Saved comparison plot to: {pdf_file}")
    
    plt.show()
    
    # Create individual metric plots
    create_individual_metric_plots(baseline_df, bidirectional_df, output_dir)
    
    # Generate summary statistics
    generate_summary_statistics(baseline_df, bidirectional_df, output_dir)

def create_individual_metric_plots(baseline_df, bidirectional_df, output_dir):
    """Create individual plots for each metric."""
    
    metrics_to_plot = [
        ('Hits@1', 'Hits@1'),
        ('Hits@3', 'Hits@3'), 
        ('Hits@5', 'Hits@5'),
        ('Hits@10', 'Hits@10'),
        ('MRR', 'Mean Reciprocal Rank'),
        ('Mean_Rank', 'Mean Rank')
    ]
    
    for metric_col, metric_title in metrics_to_plot:
        plt.figure(figsize=(10, 6))
        
        # Plot baseline
        if baseline_df is not None and metric_col in baseline_df.columns:
            plt.plot(baseline_df['epoch'], baseline_df[metric_col], 
                    'o-', color='blue', linewidth=2, markersize=6, 
                    label='Baseline', alpha=0.8)
        
        # Plot bidirectional
        if bidirectional_df is not None and metric_col in bidirectional_df.columns:
            plt.plot(bidirectional_df['epoch'], bidirectional_df[metric_col], 
                    's-', color='orange', linewidth=2, markersize=6, 
                    label='Bidirectional', alpha=0.8)
        
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel(metric_title, fontsize=12)
        plt.title(f'{metric_title} Evolution During Training', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        
        # Save individual plot
        safe_name = metric_col.replace('@', '_at_').replace('_', '_').lower()
        individual_file = os.path.join(output_dir, f'{safe_name}_comparison.png')
        plt.savefig(individual_file, dpi=300, bbox_inches='tight')
        print(f"Saved {metric_title} plot to: {individual_file}")
        
        plt.close()

def generate_summary_statistics(baseline_df, bidirectional_df, output_dir):
    """Generate summary statistics comparing the models."""
    
    summary_file = os.path.join(output_dir, 'training_summary.txt')
    
    with open(summary_file, 'w') as f:
        f.write("=== ComplEx Training Comparison Summary ===\n\n")
        
        # Training info
        if baseline_df is not None:
            f.write(f"Baseline training epochs: {len(baseline_df)}\n")
        if bidirectional_df is not None:
            f.write(f"Bidirectional training epochs: {len(bidirectional_df)}\n")
        f.write("\n")
        
        # Final metrics comparison
        f.write("FINAL EPOCH METRICS COMPARISON:\n")
        f.write("=" * 50 + "\n")
        
        metrics = ['Hits@1', 'Hits@3', 'Hits@5', 'Hits@10', 'MRR', 'Mean_Rank']
        
        for metric in metrics:
            baseline_final = None
            bidirectional_final = None
            
            if baseline_df is not None and metric in baseline_df.columns and len(baseline_df) > 0:
                baseline_final = baseline_df[metric].iloc[-1]
            
            if bidirectional_df is not None and metric in bidirectional_df.columns and len(bidirectional_df) > 0:
                bidirectional_final = bidirectional_df[metric].iloc[-1]
            
            f.write(f"\n{metric}:\n")
            if baseline_final is not None:
                f.write(f"  Baseline:      {baseline_final:.4f}\n")
            if bidirectional_final is not None:
                f.write(f"  Bidirectional: {bidirectional_final:.4f}\n")
            
            if baseline_final is not None and bidirectional_final is not None:
                if metric == 'Mean_Rank':
                    # Lower is better for Mean Rank
                    improvement = ((baseline_final - bidirectional_final) / baseline_final) * 100
                    if improvement > 0:
                        f.write(f"  Improvement:   {improvement:.2f}% (bidirectional is better)\n")
                    else:
                        f.write(f"  Degradation:   {abs(improvement):.2f}% (baseline is better)\n")
                else:
                    # Higher is better for Hits and MRR
                    improvement = ((bidirectional_final - baseline_final) / baseline_final) * 100
                    if improvement > 0:
                        f.write(f"  Improvement:   +{improvement:.2f}% (bidirectional is better)\n")
                    else:
                        f.write(f"  Degradation:   {improvement:.2f}% (baseline is better)\n")
        
        # Best epoch analysis
        f.write(f"\n\nBEST PERFORMANCE ANALYSIS:\n")
        f.write("=" * 50 + "\n")
        
        for metric in metrics:
            if metric == 'Mean_Rank':
                # Lower is better for Mean Rank
                if baseline_df is not None and metric in baseline_df.columns:
                    best_baseline = baseline_df[metric].min()
                    best_baseline_epoch = baseline_df.loc[baseline_df[metric].idxmin(), 'epoch']
                    f.write(f"\n{metric} - Baseline best: {best_baseline:.4f} (epoch {best_baseline_epoch})\n")
                
                if bidirectional_df is not None and metric in bidirectional_df.columns:
                    best_bidirectional = bidirectional_df[metric].min()
                    best_bidirectional_epoch = bidirectional_df.loc[bidirectional_df[metric].idxmin(), 'epoch']
                    f.write(f"{metric} - Bidirectional best: {best_bidirectional:.4f} (epoch {best_bidirectional_epoch})\n")
            else:
                # Higher is better for Hits and MRR
                if baseline_df is not None and metric in baseline_df.columns:
                    best_baseline = baseline_df[metric].max()
                    best_baseline_epoch = baseline_df.loc[baseline_df[metric].idxmax(), 'epoch']
                    f.write(f"\n{metric} - Baseline best: {best_baseline:.4f} (epoch {best_baseline_epoch})\n")
                
                if bidirectional_df is not None and metric in bidirectional_df.columns:
                    best_bidirectional = bidirectional_df[metric].max()
                    best_bidirectional_epoch = bidirectional_df.loc[bidirectional_df[metric].idxmax(), 'epoch']
                    f.write(f"{metric} - Bidirectional best: {best_bidirectional:.4f} (epoch {best_bidirectional_epoch})\n")
    
    print(f"Saved training summary to: {summary_file}")

def main():
    """Parse command line arguments and create comparison plots."""
    parser = argparse.ArgumentParser(description="Plot training metrics comparison")
    parser.add_argument("--baseline-csv", type=str, 
                        default="models/baseline_complex_with_callbacks/baseline_epoch_metrics.csv",
                        help="Path to baseline metrics CSV file")
    parser.add_argument("--bidirectional-csv", type=str, 
                        default="models/bidirectional_complex_with_callbacks/bidirectional_epoch_metrics.csv",
                        help="Path to bidirectional metrics CSV file")
    parser.add_argument("--output-dir", type=str, default="metrics_comparison_plots",
                        help="Output directory for plots and summary")
    
    args = parser.parse_args()
    
    create_training_comparison_plots(
        baseline_csv=args.baseline_csv,
        bidirectional_csv=args.bidirectional_csv,
        output_dir=args.output_dir
    )

if __name__ == "__main__":
    main() 